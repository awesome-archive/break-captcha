{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sknn.mlp import Classifier, Layer\n",
    "from sknn.mlp import Classifier, Convolution, FastVectorSpace, Layer, MultiLayerPerceptron\n",
    "import numpy as np\n",
    "from time import time\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Plan2 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "escape time :  0.063 s\n",
      "the shape of training set 5400 rows, 784 columns\n",
      "the shape of test set 600 rows, 784 columns\n",
      "the range of training set : 0.0 ~ 0.999996\n",
      "the range of test set : 0.0 ~ 0.999996\n"
     ]
    }
   ],
   "source": [
    "features = joblib.load(\"./mldata/features_1200.mat\")\n",
    "labels = joblib.load(\"./mldata/lables_1200.mat\")\n",
    "\n",
    "features = np.array(features, 'int16')\n",
    "labels = np.array(labels, 'int')\n",
    "\n",
    "t0 = time()\n",
    "def scale(X, eps = 0.001):\n",
    "    # scale the data points s.t the columns of the feature space\n",
    "    # (i.e the predictors) are within the range [0, 1]\n",
    "    return (X - np.min(X, axis = 0)) / (np.max(X, axis = 0) + eps)\n",
    "\n",
    "features = features.astype(\"float32\")\n",
    "features = scale(features)\n",
    "\n",
    "print \"escape time : \", round(time()-t0, 3), \"s\"\n",
    "\n",
    "# scale the data to the range [0, 1] and then construct the training\n",
    "# and testing splits\n",
    "(trainX, testX, trainY, testY) = train_test_split(features, labels, test_size = 0.1)\n",
    "print \"the shape of training set %s rows, %s columns\" %(trainX.shape[0], trainX.shape[1])\n",
    "print \"the shape of test set %s rows, %s columns\" %(testX.shape[0], testX.shape[1])\n",
    "print \"the range of training set : %s ~ %s\" %(trainX.min(),trainX.max())\n",
    "print \"the range of test set : %s ~ %s\" %(testX.min(),testX.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.925\n",
      "classification report : \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.94      0.94        64\n",
      "          1       0.92      0.95      0.93        96\n",
      "          2       0.88      0.91      0.89        56\n",
      "          3       0.92      0.87      0.90        55\n",
      "          4       0.99      0.93      0.96        75\n",
      "          5       0.87      0.97      0.92        62\n",
      "          6       0.91      0.71      0.79        41\n",
      "          7       0.95      1.00      0.97        92\n",
      "          8       0.93      0.92      0.92        59\n",
      "\n",
      "avg / total       0.93      0.93      0.92       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification \n",
    "# Rectifier, Sigmoid, Tanh, and Maxout for non-linear layers\n",
    "# Linear, Softmax or Gaussian for linear layers\n",
    "nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Rectifier\", units=300),\n",
    "        Layer(\"Softmax\")],\n",
    "    learning_rate=0.01,\n",
    "    n_iter=10)\n",
    "nn.fit(trainX, trainY)\n",
    "\n",
    "# compute the predictions for the test data and show a classification report\n",
    "preds = nn.predict(testX)\n",
    "\n",
    "print \"accuracy score : %s\" %(accuracy_score(testY, preds))\n",
    "print \"classification report : \"\n",
    "print classification_report(testY, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.923333333333\n",
      "classification report : \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.95      0.95        64\n",
      "          1       0.90      0.95      0.92        96\n",
      "          2       0.91      0.89      0.90        56\n",
      "          3       0.96      0.87      0.91        55\n",
      "          4       0.99      0.93      0.96        75\n",
      "          5       0.84      0.92      0.88        62\n",
      "          6       0.97      0.71      0.82        41\n",
      "          7       0.92      1.00      0.96        92\n",
      "          8       0.93      0.95      0.94        59\n",
      "\n",
      "avg / total       0.93      0.92      0.92       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification \n",
    "# Rectifier, Sigmoid, Tanh, and Maxout for non-linear layers\n",
    "# Linear, Softmax or Gaussian for linear layers\n",
    "nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Sigmoid\", units=300),\n",
    "        Layer(\"Softmax\")],\n",
    "    learning_rate=0.01,\n",
    "    n_iter=10)\n",
    "nn.fit(trainX, trainY)\n",
    "\n",
    "# compute the predictions for the test data and show a classification report\n",
    "preds = nn.predict(testX)\n",
    "\n",
    "print \"accuracy score : %s\" %(accuracy_score(testY, preds))\n",
    "print \"classification report : \"\n",
    "print classification_report(testY, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.928333333333\n",
      "classification report : \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.94      0.94        64\n",
      "          1       0.92      0.95      0.93        96\n",
      "          2       0.96      0.89      0.93        56\n",
      "          3       0.89      0.91      0.90        55\n",
      "          4       0.99      0.93      0.96        75\n",
      "          5       0.86      0.95      0.90        62\n",
      "          6       0.94      0.71      0.81        41\n",
      "          7       0.93      1.00      0.96        92\n",
      "          8       0.93      0.95      0.94        59\n",
      "\n",
      "avg / total       0.93      0.93      0.93       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification \n",
    "# Rectifier, Sigmoid, Tanh, and Maxout for non-linear layers\n",
    "# Linear, Softmax or Gaussian for linear layers\n",
    "nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Tanh\", units=300),\n",
    "        Layer(\"Softmax\")],\n",
    "    learning_rate=0.01,\n",
    "    n_iter=10)\n",
    "nn.fit(trainX, trainY)\n",
    "\n",
    "# compute the predictions for the test data and show a classification report\n",
    "preds = nn.predict(testX)\n",
    "\n",
    "print \"accuracy score : %s\" %(accuracy_score(testY, preds))\n",
    "print \"classification report : \"\n",
    "print classification_report(testY, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.928333333333\n",
      "classification report : \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.95      0.94        64\n",
      "          1       0.91      0.95      0.93        96\n",
      "          2       0.91      0.91      0.91        56\n",
      "          3       0.91      0.89      0.90        55\n",
      "          4       0.99      0.93      0.96        75\n",
      "          5       0.92      0.92      0.92        62\n",
      "          6       0.88      0.73      0.80        41\n",
      "          7       0.93      1.00      0.96        92\n",
      "          8       0.97      0.95      0.96        59\n",
      "\n",
      "avg / total       0.93      0.93      0.93       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification \n",
    "# Rectifier, Sigmoid, Tanh, and Maxout for non-linear layers\n",
    "# Linear, Softmax or Gaussian for linear layers\n",
    "nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Maxout\", units=300, pieces=2),\n",
    "        Layer(\"Softmax\")],\n",
    "    learning_rate=0.01,\n",
    "    n_iter=10)\n",
    "nn.fit(trainX, trainY)\n",
    "\n",
    "# compute the predictions for the test data and show a classification report\n",
    "preds = nn.predict(testX)\n",
    "\n",
    "print \"accuracy score : %s\" %(accuracy_score(testY, preds))\n",
    "print \"classification report : \"\n",
    "print classification_report(testY, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.161666666667\n",
      "classification report : \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        61\n",
      "          1       0.16      1.00      0.28        97\n",
      "          2       0.00      0.00      0.00        73\n",
      "          3       0.00      0.00      0.00        59\n",
      "          4       0.00      0.00      0.00        64\n",
      "          5       0.00      0.00      0.00        56\n",
      "          6       0.00      0.00      0.00        44\n",
      "          7       0.00      0.00      0.00        88\n",
      "          8       0.00      0.00      0.00        58\n",
      "\n",
      "avg / total       0.03      0.16      0.04       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grid Search\n",
    "# Rectifier, Sigmoid, Tanh, and Maxout for non-linear layers\n",
    "# Linear, Softmax or Gaussian for linear layers\n",
    "\n",
    "nn = Regressor(\n",
    "    layers=[\n",
    "        Layer(\"Rectifier\", units=300),# 첫번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=300),# 두번째 히든레이어\n",
    "        Layer(\"Softmax\")], # 아웃풋 레이어\n",
    "    verbose=1)\n",
    "\n",
    "gs = GridSearchCV(nn, param_grid={\n",
    "    'learning_rate': [0.01, 0.05],\n",
    "    'n_iter' : [10, 20],\n",
    "    'hidden0__units': [300, 400],\n",
    "    'hidden0__type': [\"Rectifier\", \"Tanh\"], # 첫번째 히든레이어\n",
    "    'hidden1__units': [300, 400],\n",
    "    'hidden1__type': [\"Rectifier\", \"Tanh\"]}) # 두번째 히든레이어\n",
    "gs.fit(trainX, trainY)\n",
    "\n",
    "\n",
    "# compute the predictions for the test data and show a classification report\n",
    "preds = gs.predict(testX)\n",
    "\n",
    "print \"accuracy score : %s\" %(accuracy_score(testY, preds))\n",
    "print \"classification report : \"\n",
    "print classification_report(testY, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.93\n",
      "classification report : \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.96      0.96        67\n",
      "          1       0.92      0.94      0.93       105\n",
      "          2       0.98      0.90      0.94        61\n",
      "          3       0.94      0.91      0.92        53\n",
      "          4       0.95      0.94      0.95        86\n",
      "          5       0.88      0.98      0.93        59\n",
      "          6       0.71      0.80      0.75        30\n",
      "          7       0.99      0.92      0.95        76\n",
      "          8       0.95      0.94      0.94        63\n",
      "\n",
      "avg / total       0.93      0.93      0.93       600\n",
      "\n",
      "Classifier(batch_size=1, debug=False, dropout_rate=None, f_stable=0.001,\n",
      "      hidden0=<sknn.nn.Layer `Rectifier`: name=u'hidden0', units=200, weight_decay=9e-05>,\n",
      "      layers=[<sknn.nn.Layer `Rectifier`: name=u'hidden0', units=200, weight_decay=9e-05>, <sknn.nn.Layer `Softmax`: name=u'output', units=9>],\n",
      "      learning_momentum=0.9, learning_rate=0.009, learning_rule=u'sgd',\n",
      "      loss_type=u'mse', n_iter=10, n_stable=50,\n",
      "      output=<sknn.nn.Layer `Softmax`: name=u'output', units=9>,\n",
      "      random_state=None, regularize=None, valid_set=None, valid_size=0.0,\n",
      "      verbose=2, weight_decay=None)\n",
      "escape time :  2534.717 s\n"
     ]
    }
   ],
   "source": [
    "# Grid Search\n",
    "# Rectifier, Sigmoid, Tanh, and Maxout for non-linear layers\n",
    "# Linear, Softmax or Gaussian for linear layers\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Rectifier\", units=300), # 첫번째 히든레이어\n",
    "        Layer(\"Softmax\")], # 아웃풋 레이어\n",
    "    verbose=2)\n",
    "\n",
    "gs = GridSearchCV(nn, param_grid={\n",
    "    'learning_rate': [0.009],\n",
    "    'n_iter' : [10],\n",
    "    'hidden0__units': [100, 200, 300],\n",
    "    'hidden0__weight_decay' : [0.9, 0.09, 0.009, 0.0009, 0.00009],\n",
    "    'hidden0__type': [\"Rectifier\", \"Tanh\"] # 첫번째 히든레이어\n",
    "    })\n",
    "gs.fit(trainX, trainY)\n",
    "\n",
    "\n",
    "# compute the predictions for the test data and show a classification report\n",
    "preds = gs.predict(testX)\n",
    "\n",
    "print \"accuracy score : %s\" %(accuracy_score(testY, preds))\n",
    "print \"classification report : \"\n",
    "print classification_report(testY, preds)\n",
    "print gs.best_estimator_\n",
    "print \"escape time : \", round(time()-t0, 3), \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.93\n",
      "classification report : \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.96      0.96        67\n",
      "          1       0.92      0.94      0.93       105\n",
      "          2       0.98      0.90      0.94        61\n",
      "          3       0.94      0.91      0.92        53\n",
      "          4       0.95      0.94      0.95        86\n",
      "          5       0.88      0.98      0.93        59\n",
      "          6       0.71      0.80      0.75        30\n",
      "          7       0.99      0.92      0.95        76\n",
      "          8       0.95      0.94      0.94        63\n",
      "\n",
      "avg / total       0.93      0.93      0.93       600\n",
      "\n",
      "escape time :  33.315 s\n"
     ]
    }
   ],
   "source": [
    "# Rectifier, Sigmoid, Tanh, and Maxout for non-linear layers\n",
    "# Linear, Softmax or Gaussian for linear layers\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 첫번째 히든레이어\n",
    "        Layer(\"Softmax\")], # 아웃풋 레이어\n",
    "    learning_rate=0.009,\n",
    "    n_iter=10,\n",
    "    verbose=2)\n",
    "\n",
    "nn.fit(trainX, trainY)\n",
    "\n",
    "\n",
    "# compute the predictions for the test data and show a classification report\n",
    "preds = nn.predict(testX)\n",
    "\n",
    "print \"accuracy score : %s\" %(accuracy_score(testY, preds))\n",
    "print \"classification report : \"\n",
    "print classification_report(testY, preds)\n",
    "print \"escape time : \", round(time()-t0, 3), \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.931666666667\n",
      "classification report : \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.96      0.96        67\n",
      "          1       0.93      0.94      0.94       105\n",
      "          2       0.96      0.90      0.93        61\n",
      "          3       0.94      0.91      0.92        53\n",
      "          4       0.95      0.95      0.95        86\n",
      "          5       0.85      0.98      0.91        59\n",
      "          6       0.73      0.80      0.76        30\n",
      "          7       0.99      0.92      0.95        76\n",
      "          8       0.97      0.94      0.95        63\n",
      "\n",
      "avg / total       0.93      0.93      0.93       600\n",
      "\n",
      "escape time :  44.577 s\n"
     ]
    }
   ],
   "source": [
    "# Rectifier, Sigmoid, Tanh, and Maxout for non-linear layers\n",
    "# Linear, Softmax or Gaussian for linear layers\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 첫번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 두번째 히든레이어\n",
    "        Layer(\"Softmax\")], # 아웃풋 레이어\n",
    "    learning_rate=0.009,\n",
    "    n_iter=10,\n",
    "    verbose=2)\n",
    "\n",
    "nn.fit(trainX, trainY)\n",
    "\n",
    "\n",
    "# compute the predictions for the test data and show a classification report\n",
    "preds = nn.predict(testX)\n",
    "\n",
    "print \"accuracy score : %s\" %(accuracy_score(testY, preds))\n",
    "print \"classification report : \"\n",
    "print classification_report(testY, preds)\n",
    "print \"escape time : \", round(time()-t0, 3), \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.933333333333\n",
      "classification report : \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.96      0.96        67\n",
      "          1       0.93      0.95      0.94       105\n",
      "          2       1.00      0.90      0.95        61\n",
      "          3       0.94      0.91      0.92        53\n",
      "          4       0.98      0.95      0.96        86\n",
      "          5       0.88      0.97      0.92        59\n",
      "          6       0.66      0.83      0.74        30\n",
      "          7       0.99      0.92      0.95        76\n",
      "          8       0.95      0.94      0.94        63\n",
      "\n",
      "avg / total       0.94      0.93      0.93       600\n",
      "\n",
      "escape time :  57.057 s\n"
     ]
    }
   ],
   "source": [
    "# Rectifier, Sigmoid, Tanh, and Maxout for non-linear layers\n",
    "# Linear, Softmax or Gaussian for linear layers\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 첫번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 두번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 세번째 히든레이어\n",
    "        Layer(\"Softmax\")], # 아웃풋 레이어\n",
    "    learning_rate=0.009,\n",
    "    n_iter=10,\n",
    "    verbose=2)\n",
    "\n",
    "nn.fit(trainX, trainY)\n",
    "\n",
    "\n",
    "# compute the predictions for the test data and show a classification report\n",
    "preds = nn.predict(testX)\n",
    "\n",
    "print \"accuracy score : %s\" %(accuracy_score(testY, preds))\n",
    "print \"classification report : \"\n",
    "print classification_report(testY, preds)\n",
    "print \"escape time : \", round(time()-t0, 3), \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.933333333333\n",
      "classification report : \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.96      0.96        67\n",
      "          1       0.93      0.94      0.93       105\n",
      "          2       1.00      0.92      0.96        61\n",
      "          3       0.94      0.92      0.93        53\n",
      "          4       0.94      0.95      0.95        86\n",
      "          5       0.91      0.98      0.94        59\n",
      "          6       0.67      0.80      0.73        30\n",
      "          7       0.97      0.92      0.95        76\n",
      "          8       0.98      0.92      0.95        63\n",
      "\n",
      "avg / total       0.94      0.93      0.93       600\n",
      "\n",
      "escape time :  68.718 s\n"
     ]
    }
   ],
   "source": [
    "# Rectifier, Sigmoid, Tanh, and Maxout for non-linear layers\n",
    "# Linear, Softmax or Gaussian for linear layers\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 첫번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 두번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 세번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 네번째 히든레이어\n",
    "        Layer(\"Softmax\")], # 아웃풋 레이어\n",
    "    learning_rate=0.009,\n",
    "    n_iter=10,\n",
    "    verbose=2)\n",
    "\n",
    "nn.fit(trainX, trainY)\n",
    "\n",
    "\n",
    "# compute the predictions for the test data and show a classification report\n",
    "preds = nn.predict(testX)\n",
    "\n",
    "print \"accuracy score : %s\" %(accuracy_score(testY, preds))\n",
    "print \"classification report : \"\n",
    "print classification_report(testY, preds)\n",
    "print \"escape time : \", round(time()-t0, 3), \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.93\n",
      "classification report : \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.94      0.96        67\n",
      "          1       0.93      0.94      0.94       105\n",
      "          2       0.98      0.90      0.94        61\n",
      "          3       0.89      0.91      0.90        53\n",
      "          4       0.96      0.95      0.96        86\n",
      "          5       0.89      0.98      0.94        59\n",
      "          6       0.63      0.80      0.71        30\n",
      "          7       0.96      0.92      0.94        76\n",
      "          8       1.00      0.94      0.97        63\n",
      "\n",
      "avg / total       0.94      0.93      0.93       600\n",
      "\n",
      "escape time :  82.627 s\n"
     ]
    }
   ],
   "source": [
    "# Rectifier, Sigmoid, Tanh, and Maxout for non-linear layers\n",
    "# Linear, Softmax or Gaussian for linear layers\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 첫번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 두번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 세번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 네번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 다섯번째 히든레이어\n",
    "        Layer(\"Softmax\")], # 아웃풋 레이어\n",
    "    learning_rate=0.009,\n",
    "    n_iter=10,\n",
    "    verbose=2)\n",
    "\n",
    "nn.fit(trainX, trainY)\n",
    "\n",
    "\n",
    "# compute the predictions for the test data and show a classification report\n",
    "preds = nn.predict(testX)\n",
    "\n",
    "print \"accuracy score : %s\" %(accuracy_score(testY, preds))\n",
    "print \"classification report : \"\n",
    "print classification_report(testY, preds)\n",
    "print \"escape time : \", round(time()-t0, 3), \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.928333333333\n",
      "classification report : \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.94      0.95        67\n",
      "          1       0.93      0.94      0.94       105\n",
      "          2       0.96      0.89      0.92        61\n",
      "          3       0.92      0.91      0.91        53\n",
      "          4       0.96      0.94      0.95        86\n",
      "          5       0.91      0.98      0.94        59\n",
      "          6       0.67      0.80      0.73        30\n",
      "          7       0.97      0.92      0.95        76\n",
      "          8       0.92      0.95      0.94        63\n",
      "\n",
      "avg / total       0.93      0.93      0.93       600\n",
      "\n",
      "escape time :  94.971 s\n"
     ]
    }
   ],
   "source": [
    "# Rectifier, Sigmoid, Tanh, and Maxout for non-linear layers\n",
    "# Linear, Softmax or Gaussian for linear layers\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 첫번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 두번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 세번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 네번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 다섯번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 여섯번째 히든레이어\n",
    "        Layer(\"Softmax\")], # 아웃풋 레이어\n",
    "    learning_rate=0.009,\n",
    "    n_iter=10,\n",
    "    verbose=2)\n",
    "\n",
    "nn.fit(trainX, trainY)\n",
    "\n",
    "\n",
    "# compute the predictions for the test data and show a classification report\n",
    "preds = nn.predict(testX)\n",
    "\n",
    "print \"accuracy score : %s\" %(accuracy_score(testY, preds))\n",
    "print \"classification report : \"\n",
    "print classification_report(testY, preds)\n",
    "print \"escape time : \", round(time()-t0, 3), \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.926666666667\n",
      "classification report : \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.94      0.96        67\n",
      "          1       0.93      0.94      0.93       105\n",
      "          2       1.00      0.89      0.94        61\n",
      "          3       0.91      0.92      0.92        53\n",
      "          4       0.96      0.94      0.95        86\n",
      "          5       0.88      0.97      0.92        59\n",
      "          6       0.62      0.83      0.71        30\n",
      "          7       0.96      0.92      0.94        76\n",
      "          8       0.98      0.92      0.95        63\n",
      "\n",
      "avg / total       0.93      0.93      0.93       600\n",
      "\n",
      "escape time :  106.679 s\n"
     ]
    }
   ],
   "source": [
    "# Rectifier, Sigmoid, Tanh, and Maxout for non-linear layers\n",
    "# Linear, Softmax or Gaussian for linear layers\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 첫번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 두번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 세번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 네번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 다섯번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 여섯번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 7번째 히든레이어\n",
    "        Layer(\"Softmax\")], # 아웃풋 레이어\n",
    "    learning_rate=0.009,\n",
    "    n_iter=10,\n",
    "    verbose=2)\n",
    "\n",
    "nn.fit(trainX, trainY)\n",
    "\n",
    "\n",
    "# compute the predictions for the test data and show a classification report\n",
    "preds = nn.predict(testX)\n",
    "\n",
    "print \"accuracy score : %s\" %(accuracy_score(testY, preds))\n",
    "print \"classification report : \"\n",
    "print classification_report(testY, preds)\n",
    "print \"escape time : \", round(time()-t0, 3), \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.926666666667\n",
      "classification report : \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.94      0.97        67\n",
      "          1       0.93      0.93      0.93       105\n",
      "          2       0.98      0.90      0.94        61\n",
      "          3       0.96      0.91      0.93        53\n",
      "          4       0.96      0.95      0.96        86\n",
      "          5       0.83      0.98      0.90        59\n",
      "          6       0.63      0.80      0.71        30\n",
      "          7       0.97      0.92      0.95        76\n",
      "          8       0.95      0.92      0.94        63\n",
      "\n",
      "avg / total       0.93      0.93      0.93       600\n",
      "\n",
      "escape time :  115.55 s\n"
     ]
    }
   ],
   "source": [
    "# Rectifier, Sigmoid, Tanh, and Maxout for non-linear layers\n",
    "# Linear, Softmax or Gaussian for linear layers\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 첫번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 두번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 세번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 네번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 다섯번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 여섯번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 7번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 8번째 히든레이어\n",
    "        Layer(\"Softmax\")], # 아웃풋 레이어\n",
    "    learning_rate=0.009,\n",
    "    n_iter=10,\n",
    "    verbose=2)\n",
    "\n",
    "nn.fit(trainX, trainY)\n",
    "\n",
    "\n",
    "# compute the predictions for the test data and show a classification report\n",
    "preds = nn.predict(testX)\n",
    "\n",
    "print \"accuracy score : %s\" %(accuracy_score(testY, preds))\n",
    "print \"classification report : \"\n",
    "print classification_report(testY, preds)\n",
    "print \"escape time : \", round(time()-t0, 3), \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.921666666667\n",
      "classification report : \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.94      0.95        67\n",
      "          1       0.93      0.94      0.94       105\n",
      "          2       0.96      0.90      0.93        61\n",
      "          3       0.94      0.91      0.92        53\n",
      "          4       0.96      0.94      0.95        86\n",
      "          5       0.85      0.98      0.91        59\n",
      "          6       0.55      0.70      0.62        30\n",
      "          7       0.96      0.92      0.94        76\n",
      "          8       1.00      0.92      0.96        63\n",
      "\n",
      "avg / total       0.93      0.92      0.92       600\n",
      "\n",
      "escape time :  135.532 s\n"
     ]
    }
   ],
   "source": [
    "# Rectifier, Sigmoid, Tanh, and Maxout for non-linear layers\n",
    "# Linear, Softmax or Gaussian for linear layers\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 첫번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 두번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 세번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 네번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 다섯번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 여섯번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 7번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 8번째 히든레이어\n",
    "        Layer(\"Rectifier\", units=200, weight_decay=0.00009), # 9번째 히든레이어\n",
    "        Layer(\"Softmax\")], # 아웃풋 레이어\n",
    "    learning_rate=0.009,\n",
    "    n_iter=10,\n",
    "    verbose=2)\n",
    "\n",
    "nn.fit(trainX, trainY)\n",
    "\n",
    "\n",
    "# compute the predictions for the test data and show a classification report\n",
    "preds = nn.predict(testX)\n",
    "\n",
    "print \"accuracy score : %s\" %(accuracy_score(testY, preds))\n",
    "print \"classification report : \"\n",
    "print classification_report(testY, preds)\n",
    "print \"escape time : \", round(time()-t0, 3), \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.93\n",
      "classification report : \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.96      0.95        67\n",
      "          1       0.93      0.94      0.94       105\n",
      "          2       0.98      0.89      0.93        61\n",
      "          3       0.94      0.91      0.92        53\n",
      "          4       0.96      0.95      0.96        86\n",
      "          5       0.85      0.98      0.91        59\n",
      "          6       0.71      0.80      0.75        30\n",
      "          7       0.99      0.92      0.95        76\n",
      "          8       0.95      0.94      0.94        63\n",
      "\n",
      "avg / total       0.93      0.93      0.93       600\n",
      "\n",
      "escape time :  79.859 s\n"
     ]
    }
   ],
   "source": [
    "# Rectifier, Sigmoid, Tanh, and Maxout for non-linear layers\n",
    "# Linear, Softmax or Gaussian for linear layers\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 첫번째 히든레이어\n",
    "        Layer(\"Softmax\")], # 아웃풋 레이어\n",
    "    learning_rate=0.009,\n",
    "    n_iter=10,\n",
    "    verbose=2)\n",
    "\n",
    "nn.fit(trainX, trainY)\n",
    "\n",
    "\n",
    "# compute the predictions for the test data and show a classification report\n",
    "preds = nn.predict(testX)\n",
    "\n",
    "print \"accuracy score : %s\" %(accuracy_score(testY, preds))\n",
    "print \"classification report : \"\n",
    "print classification_report(testY, preds)\n",
    "print \"escape time : \", round(time()-t0, 3), \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.926666666667\n",
      "classification report : \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.97      0.93        67\n",
      "          1       0.92      0.94      0.93       105\n",
      "          2       0.98      0.89      0.93        61\n",
      "          3       0.92      0.91      0.91        53\n",
      "          4       0.96      0.94      0.95        86\n",
      "          5       0.88      0.98      0.93        59\n",
      "          6       0.71      0.73      0.72        30\n",
      "          7       0.97      0.92      0.95        76\n",
      "          8       1.00      0.94      0.97        63\n",
      "\n",
      "avg / total       0.93      0.93      0.93       600\n",
      "\n",
      "escape time :  77.639 s\n"
     ]
    }
   ],
   "source": [
    "# Rectifier, Sigmoid, Tanh, and Maxout for non-linear layers\n",
    "# Linear, Softmax or Gaussian for linear layers\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 1번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 2번째 히든레이어\n",
    "        Layer(\"Softmax\")], # 아웃풋 레이어\n",
    "    learning_rate=0.009,\n",
    "    n_iter=10,\n",
    "    verbose=2)\n",
    "\n",
    "nn.fit(trainX, trainY)\n",
    "\n",
    "\n",
    "# compute the predictions for the test data and show a classification report\n",
    "preds = nn.predict(testX)\n",
    "\n",
    "print \"accuracy score : %s\" %(accuracy_score(testY, preds))\n",
    "print \"classification report : \"\n",
    "print classification_report(testY, preds)\n",
    "print \"escape time : \", round(time()-t0, 3), \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.935\n",
      "classification report : \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.96      0.96        67\n",
      "          1       0.93      0.94      0.93       105\n",
      "          2       0.97      0.93      0.95        61\n",
      "          3       0.92      0.92      0.92        53\n",
      "          4       0.97      0.97      0.97        86\n",
      "          5       0.90      0.95      0.93        59\n",
      "          6       0.69      0.80      0.74        30\n",
      "          7       0.97      0.92      0.95        76\n",
      "          8       0.98      0.94      0.96        63\n",
      "\n",
      "avg / total       0.94      0.94      0.94       600\n",
      "\n",
      "escape time :  120.764 s\n"
     ]
    }
   ],
   "source": [
    "# Rectifier, Sigmoid, Tanh, and Maxout for non-linear layers\n",
    "# Linear, Softmax or Gaussian for linear layers\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 1번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 2번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 3번째 히든레이어\n",
    "        Layer(\"Softmax\")], # 아웃풋 레이어\n",
    "    learning_rate=0.0009,\n",
    "    n_iter=10,\n",
    "    verbose=2)\n",
    "\n",
    "nn.fit(trainX, trainY)\n",
    "\n",
    "\n",
    "# compute the predictions for the test data and show a classification report\n",
    "preds = nn.predict(testX)\n",
    "\n",
    "print \"accuracy score : %s\" %(accuracy_score(testY, preds))\n",
    "print \"classification report : \"\n",
    "print classification_report(testY, preds)\n",
    "print \"escape time : \", round(time()-t0, 3), \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.926666666667\n",
      "classification report : \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.96      0.96        67\n",
      "          1       0.93      0.94      0.93       105\n",
      "          2       0.95      0.90      0.92        61\n",
      "          3       0.89      0.92      0.91        53\n",
      "          4       0.93      0.97      0.95        86\n",
      "          5       0.88      0.90      0.89        59\n",
      "          6       0.73      0.80      0.76        30\n",
      "          7       0.99      0.92      0.95        76\n",
      "          8       0.98      0.94      0.96        63\n",
      "\n",
      "avg / total       0.93      0.93      0.93       600\n",
      "\n",
      "escape time :  159.644 s\n"
     ]
    }
   ],
   "source": [
    "# Rectifier, Sigmoid, Tanh, and Maxout for non-linear layers\n",
    "# Linear, Softmax or Gaussian for linear layers\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 1번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 2번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 3번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 4번째 히든레이어\n",
    "        Layer(\"Softmax\")], # 아웃풋 레이어\n",
    "    learning_rate=0.0009,\n",
    "    n_iter=10,\n",
    "    verbose=2)\n",
    "\n",
    "nn.fit(trainX, trainY)\n",
    "\n",
    "\n",
    "# compute the predictions for the test data and show a classification report\n",
    "preds = nn.predict(testX)\n",
    "\n",
    "print \"accuracy score : %s\" %(accuracy_score(testY, preds))\n",
    "print \"classification report : \"\n",
    "print classification_report(testY, preds)\n",
    "print \"escape time : \", round(time()-t0, 3), \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.925\n",
      "classification report : \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.97      0.96        67\n",
      "          1       0.94      0.94      0.94       105\n",
      "          2       0.93      0.93      0.93        61\n",
      "          3       0.89      0.91      0.90        53\n",
      "          4       0.96      0.94      0.95        86\n",
      "          5       0.87      0.90      0.88        59\n",
      "          6       0.69      0.80      0.74        30\n",
      "          7       0.97      0.92      0.95        76\n",
      "          8       0.98      0.92      0.95        63\n",
      "\n",
      "avg / total       0.93      0.93      0.93       600\n",
      "\n",
      "escape time :  157.876 s\n"
     ]
    }
   ],
   "source": [
    "# Rectifier, Sigmoid, Tanh, and Maxout for non-linear layers\n",
    "# Linear, Softmax or Gaussian for linear layers\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 1번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 2번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 3번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 4번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 5번째 히든레이어\n",
    "        Layer(\"Softmax\")], # 아웃풋 레이어\n",
    "    learning_rate=0.0009,\n",
    "    n_iter=10,\n",
    "    verbose=2)\n",
    "\n",
    "nn.fit(trainX, trainY)\n",
    "\n",
    "\n",
    "# compute the predictions for the test data and show a classification report\n",
    "preds = nn.predict(testX)\n",
    "\n",
    "print \"accuracy score : %s\" %(accuracy_score(testY, preds))\n",
    "print \"classification report : \"\n",
    "print classification_report(testY, preds)\n",
    "print \"escape time : \", round(time()-t0, 3), \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.936666666667\n",
      "classification report : \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.97      0.97        67\n",
      "          1       0.93      0.94      0.94       105\n",
      "          2       0.98      0.93      0.96        61\n",
      "          3       0.93      0.94      0.93        53\n",
      "          4       0.95      0.95      0.95        86\n",
      "          5       0.90      0.97      0.93        59\n",
      "          6       0.66      0.77      0.71        30\n",
      "          7       0.99      0.92      0.95        76\n",
      "          8       0.98      0.94      0.96        63\n",
      "\n",
      "avg / total       0.94      0.94      0.94       600\n",
      "\n",
      "escape time :  185.98 s\n"
     ]
    }
   ],
   "source": [
    "# Rectifier, Sigmoid, Tanh, and Maxout for non-linear layers\n",
    "# Linear, Softmax or Gaussian for linear layers\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 1번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 2번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 3번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 4번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 5번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 6번째 히든레이어\n",
    "        Layer(\"Softmax\")], # 아웃풋 레이어\n",
    "    learning_rate=0.0009,\n",
    "    n_iter=10,\n",
    "    verbose=2)\n",
    "\n",
    "nn.fit(trainX, trainY)\n",
    "\n",
    "\n",
    "# compute the predictions for the test data and show a classification report\n",
    "preds = nn.predict(testX)\n",
    "\n",
    "print \"accuracy score : %s\" %(accuracy_score(testY, preds))\n",
    "print \"classification report : \"\n",
    "print classification_report(testY, preds)\n",
    "print \"escape time : \", round(time()-t0, 3), \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.925\n",
      "classification report : \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.96      0.93        67\n",
      "          1       0.94      0.94      0.94       105\n",
      "          2       0.89      0.90      0.89        61\n",
      "          3       0.89      0.91      0.90        53\n",
      "          4       0.96      0.95      0.96        86\n",
      "          5       0.92      0.93      0.92        59\n",
      "          6       0.80      0.80      0.80        30\n",
      "          7       0.97      0.92      0.95        76\n",
      "          8       0.95      0.92      0.94        63\n",
      "\n",
      "avg / total       0.93      0.93      0.93       600\n",
      "\n",
      "escape time :  230.662 s\n"
     ]
    }
   ],
   "source": [
    "# Rectifier, Sigmoid, Tanh, and Maxout for non-linear layers\n",
    "# Linear, Softmax or Gaussian for linear layers\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 1번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 2번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 3번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 4번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 5번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 6번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 7번째 히든레이어\n",
    "        Layer(\"Softmax\")], # 아웃풋 레이어\n",
    "    learning_rate=0.0009,\n",
    "    n_iter=10,\n",
    "    verbose=2)\n",
    "\n",
    "nn.fit(trainX, trainY)\n",
    "\n",
    "\n",
    "# compute the predictions for the test data and show a classification report\n",
    "preds = nn.predict(testX)\n",
    "\n",
    "print \"accuracy score : %s\" %(accuracy_score(testY, preds))\n",
    "print \"classification report : \"\n",
    "print classification_report(testY, preds)\n",
    "print \"escape time : \", round(time()-t0, 3), \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.921666666667\n",
      "classification report : \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.96      0.96        67\n",
      "          1       0.92      0.94      0.93       105\n",
      "          2       0.92      0.92      0.92        61\n",
      "          3       0.87      0.89      0.88        53\n",
      "          4       0.96      0.94      0.95        86\n",
      "          5       0.92      0.92      0.92        59\n",
      "          6       0.70      0.77      0.73        30\n",
      "          7       0.97      0.92      0.95        76\n",
      "          8       0.95      0.94      0.94        63\n",
      "\n",
      "avg / total       0.92      0.92      0.92       600\n",
      "\n",
      "escape time :  243.594 s\n"
     ]
    }
   ],
   "source": [
    "# Rectifier, Sigmoid, Tanh, and Maxout for non-linear layers\n",
    "# Linear, Softmax or Gaussian for linear layers\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.0009, pieces=2), # 1번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.0009, pieces=2), # 2번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.0009, pieces=2), # 3번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.0009, pieces=2), # 4번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.0009, pieces=2), # 5번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.0009, pieces=2), # 6번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.0009, pieces=2), # 7번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.0009, pieces=2), # 8번째 히든레이어\n",
    "        Layer(\"Softmax\")], # 아웃풋 레이어\n",
    "    learning_rate=0.00009,\n",
    "    n_iter=10,\n",
    "    verbose=2)\n",
    "\n",
    "nn.fit(trainX, trainY)\n",
    "\n",
    "\n",
    "# compute the predictions for the test data and show a classification report\n",
    "preds = nn.predict(testX)\n",
    "\n",
    "print \"accuracy score : %s\" %(accuracy_score(testY, preds))\n",
    "print \"classification report : \"\n",
    "print classification_report(testY, preds)\n",
    "print \"escape time : \", round(time()-t0, 3), \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.923333333333\n",
      "classification report : \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.94      0.94        67\n",
      "          1       0.90      0.94      0.92       105\n",
      "          2       0.89      0.90      0.89        61\n",
      "          3       0.98      0.89      0.93        53\n",
      "          4       0.98      0.94      0.96        86\n",
      "          5       0.88      0.95      0.91        59\n",
      "          6       0.71      0.80      0.75        30\n",
      "          7       0.97      0.92      0.95        76\n",
      "          8       0.98      0.94      0.96        63\n",
      "\n",
      "avg / total       0.93      0.92      0.92       600\n",
      "\n",
      "escape time :  278.269 s\n"
     ]
    }
   ],
   "source": [
    "# Rectifier, Sigmoid, Tanh, and Maxout for non-linear layers\n",
    "# Linear, Softmax or Gaussian for linear layers\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 1번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 2번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 3번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 4번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 5번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 6번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 7번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 8번째 히든레이어\n",
    "        Layer(\"Maxout\", units=200, weight_decay=0.00009, pieces=2), # 9번째 히든레이어\n",
    "        Layer(\"Softmax\")], # 아웃풋 레이어\n",
    "    learning_rate=0.00009,\n",
    "    n_iter=10,\n",
    "    verbose=2)\n",
    "\n",
    "nn.fit(trainX, trainY)\n",
    "\n",
    "\n",
    "# compute the predictions for the test data and show a classification report\n",
    "preds = nn.predict(testX)\n",
    "\n",
    "print \"accuracy score : %s\" %(accuracy_score(testY, preds))\n",
    "print \"classification report : \"\n",
    "print classification_report(testY, preds)\n",
    "print \"escape time : \", round(time()-t0, 3), \"s\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
